{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5EsEILRiKbk",
        "outputId": "a904a986-03d9-4720-fa9c-50590ad1e10a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: utils in /usr/local/lib/python3.10/dist-packages (1.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install einops\n",
        "!pip install utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bWkgDZAs39O",
        "outputId": "c1ce23b1-85aa-4a17-f444-e03e84755a6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
            "Cuda compilation tools, release 11.8, V11.8.89\n",
            "Build cuda_11.8.r11.8/compiler.31833905_0\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "BKU6iyQuOp1W",
        "outputId": "a12bc294-9d32-408e-f83e-c2a4daed17bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-3c365f0e9091>:10: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
            "  set_matplotlib_formats('svg', 'pdf') # For export\n",
            "INFO:lightning_fabric.utilities.seed:Seed set to 42\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda:0\n",
            "Number of workers: 2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found pretrained model at ../saved_models/tutorial17/SimCLR.ckpt, loading...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.3.4 to v2.1.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../saved_models/tutorial17/SimCLR.ckpt`\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "os.chdir('/content/drive/My Drive/TTIC DL Final Project')\n",
        "\n",
        "%run SimCLR_Functions.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2fvdnczGAtLj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms, datasets\n",
        "from einops import rearrange\n",
        "from einops.layers.torch import Rearrange\n",
        "from einops import rearrange\n",
        "from einops.layers.torch import Rearrange\n",
        "\n",
        "# helpers\n",
        "\n",
        "def pair(t):\n",
        "    return t if isinstance(t, tuple) else (t, t)\n",
        "\n",
        "def posemb_sincos_2d(h, w, dim, temperature: int = 10000, dtype = torch.float32):\n",
        "    y, x = torch.meshgrid(torch.arange(h), torch.arange(w), indexing=\"ij\")\n",
        "    assert (dim % 4) == 0, \"feature dimension must be multiple of 4 for sincos emb\"\n",
        "    omega = torch.arange(dim // 4) / (dim // 4 - 1)\n",
        "    omega = 1.0 / (temperature ** omega)\n",
        "\n",
        "    y = y.flatten()[:, None] * omega[None, :]\n",
        "    x = x.flatten()[:, None] * omega[None, :]\n",
        "    pe = torch.cat((x.sin(), x.cos(), y.sin(), y.cos()), dim=1)\n",
        "    return pe.type(dtype)\n",
        "\n",
        "# classes\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden_dim, dim),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, heads = 8, dim_head = 64):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head *  heads\n",
        "        self.heads = heads\n",
        "        self.scale = dim_head ** -0.5\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "\n",
        "        self.attend = nn.Softmax(dim = -1)\n",
        "\n",
        "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
        "        self.to_out = nn.Linear(inner_dim, dim, bias = False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.norm(x)\n",
        "\n",
        "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
        "\n",
        "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
        "\n",
        "        attn = self.attend(dots)\n",
        "\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        return self.to_out(out)\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, dim, depth, heads, dim_head, mlp_dim):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                Attention(dim, heads = heads, dim_head = dim_head),\n",
        "                FeedForward(dim, mlp_dim)\n",
        "            ]))\n",
        "    def forward(self, x):\n",
        "        for attn, ff in self.layers:\n",
        "            x = attn(x) + x\n",
        "            x = ff(x) + x\n",
        "        return self.norm(x)\n",
        "\n",
        "class SimCLRViT(nn.Module):\n",
        "  # Added simclr_encoder\n",
        "    def __init__(self, simclr_model, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, channels = 3, dim_head = 64):\n",
        "        super().__init__()\n",
        "        image_height, image_width = pair(image_size)\n",
        "        patch_height, patch_width = pair(patch_size)\n",
        "        self.simclr_model = simclr_model\n",
        "        self.num_patches = (image_size // patch_size) ** 2\n",
        "\n",
        "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
        "\n",
        "        patch_dim = channels * patch_height * patch_width\n",
        "\n",
        "        self.patching = nn.Sequential(\n",
        "            nn.Linear(128//self.num_patches, 256)\n",
        "        )\n",
        "\n",
        "        self.pos_embedding = posemb_sincos_2d(\n",
        "            h = image_height // patch_height,\n",
        "            w = image_width // patch_width,\n",
        "            dim = dim,\n",
        "        )\n",
        "\n",
        "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim)\n",
        "\n",
        "        self.pool = \"mean\"\n",
        "        self.to_latent = nn.Identity()\n",
        "\n",
        "        self.linear_head = nn.Linear(dim, num_classes)\n",
        "\n",
        "    def forward(self, img):\n",
        "        device = img.device\n",
        "\n",
        "        # Apply simclr encoder\n",
        "        x = self.simclr_model.convnet(img).view(-1, 128, 1)\n",
        "        b, _, _ = x.shape\n",
        "\n",
        "        # Segment the encoding as a \"patching\" operation\n",
        "        x = x.view(b, self.num_patches, -1)\n",
        "        x = self.patching(x)\n",
        "\n",
        "        x = x.view(-1, self.num_patches, 256)\n",
        "        x += self.pos_embedding.to(device, dtype=x.dtype)\n",
        "\n",
        "        x = self.transformer(x)\n",
        "        x = x.mean(dim = 1)\n",
        "\n",
        "        x = self.to_latent(x)\n",
        "        return self.linear_head(x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleViTDrop(nn.Module):\n",
        "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, channels = 3, dim_head = 64):\n",
        "        super().__init__()\n",
        "        image_height, image_width = pair(image_size)\n",
        "        patch_height, patch_width = pair(patch_size)\n",
        "        self.num_patches = (image_size // patch_size) ** 2\n",
        "\n",
        "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
        "\n",
        "        patch_dim = channels * patch_height * patch_width\n",
        "\n",
        "        self.to_patch_embedding = nn.Sequential(\n",
        "            Rearrange(\"b c (h p1) (w p2) -> b (h w) (p1 p2 c)\", p1 = patch_height, p2 = patch_width),\n",
        "            nn.LayerNorm(patch_dim),\n",
        "            nn.Linear(patch_dim, dim),\n",
        "            nn.LayerNorm(dim),\n",
        "        )\n",
        "\n",
        "        self.pos_embedding = posemb_sincos_2d(\n",
        "            h = image_height // patch_height,\n",
        "            w = image_width // patch_width,\n",
        "            dim = dim,\n",
        "        )\n",
        "\n",
        "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim)\n",
        "\n",
        "        self.pool = \"mean\"\n",
        "        self.to_latent = nn.Identity()\n",
        "\n",
        "        self.linear_head = nn.Linear(dim, num_classes)\n",
        "\n",
        "    def forward(self, img):\n",
        "        device = img.device\n",
        "        # img is [b, 3, 32, 32]\n",
        "        x = self.to_patch_embedding(img) # Now [b, num_patches, 256]\n",
        "\n",
        "        perm = torch.randperm(self.num_patches)\n",
        "        # Select num_patches // 2 patches for each image in the batch\n",
        "        x = x[:, perm[:self.num_patches // 2], :]\n",
        "\n",
        "        y = self.pos_embedding.to(device, dtype=x.dtype)[perm[:self.num_patches // 2], :]\n",
        "        x += y\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        x = x.mean(dim = 1)\n",
        "\n",
        "        x = self.to_latent(x)\n",
        "        return self.linear_head(x)"
      ],
      "metadata": {
        "id": "-M4z5Csb5jtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleViT(nn.Module):\n",
        "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, channels = 3, dim_head = 64):\n",
        "        super().__init__()\n",
        "        image_height, image_width = pair(image_size)\n",
        "        patch_height, patch_width = pair(patch_size)\n",
        "        self.num_patches = (image_size // patch_size) ** 2\n",
        "\n",
        "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
        "\n",
        "        patch_dim = channels * patch_height * patch_width\n",
        "\n",
        "        self.to_patch_embedding = nn.Sequential(\n",
        "            Rearrange(\"b c (h p1) (w p2) -> b (h w) (p1 p2 c)\", p1 = patch_height, p2 = patch_width),\n",
        "            nn.LayerNorm(patch_dim),\n",
        "            nn.Linear(patch_dim, dim),\n",
        "            nn.LayerNorm(dim),\n",
        "        )\n",
        "\n",
        "        self.pos_embedding = posemb_sincos_2d(\n",
        "            h = image_height // patch_height,\n",
        "            w = image_width // patch_width,\n",
        "            dim = dim,\n",
        "        )\n",
        "\n",
        "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim)\n",
        "\n",
        "        self.pool = \"mean\"\n",
        "        self.to_latent = nn.Identity()\n",
        "\n",
        "        self.linear_head = nn.Linear(dim, num_classes)\n",
        "\n",
        "    def forward(self, img):\n",
        "        device = img.device\n",
        "        # img is [b, 3, 32, 32]\n",
        "        x = self.to_patch_embedding(img) # Now [b, num_patches, 256]\n",
        "\n",
        "        y = self.pos_embedding.to(device, dtype=x.dtype)\n",
        "        x += y\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        x = x.mean(dim = 1)\n",
        "\n",
        "        x = self.to_latent(x)\n",
        "        return self.linear_head(x)"
      ],
      "metadata": {
        "id": "Y6uMN43AyCse"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimCLRViT2(nn.Module):\n",
        "    def __init__(self, simclr_model, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, channels = 3, dim_head = 64):\n",
        "        super().__init__()\n",
        "        image_height, image_width = pair(image_size)\n",
        "        patch_height, patch_width = pair(patch_size)\n",
        "\n",
        "        self.patch_size = patch_size\n",
        "        self.channels = channels\n",
        "        self.simclr_model = simclr_model\n",
        "\n",
        "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
        "\n",
        "        patch_dim = channels * patch_height * patch_width\n",
        "\n",
        "        self.to_patches = nn.Sequential(\n",
        "            Rearrange(\"b c (h p1) (w p2) -> b (h w) c p1 p2\", p1 = patch_height, p2 = patch_width),\n",
        "            # h x w == number of total patches, c == channels, b == number of batches\n",
        "            nn.LayerNorm(normalized_shape=(self.channels, self.patch_size, self.patch_size), elementwise_affine=True)\n",
        "            )\n",
        "\n",
        "        self.increase_dim = nn.Sequential(\n",
        "            nn.Linear(in_features=128, out_features=256),\n",
        "            nn.LayerNorm(256), # added\n",
        "        )\n",
        "\n",
        "        self.pos_embedding = posemb_sincos_2d(\n",
        "            h = image_height // patch_height,\n",
        "            w = image_width // patch_width,\n",
        "            dim = dim,\n",
        "        )\n",
        "\n",
        "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim)\n",
        "\n",
        "        self.pool = \"mean\"\n",
        "        self.to_latent = nn.Identity()\n",
        "\n",
        "        self.linear_head = nn.Linear(dim, num_classes)\n",
        "\n",
        "    def forward(self, img):\n",
        "        device = img.device # img has shape [b, 3, 32, 32], b = 64\n",
        "        patches = self.to_patches(img) # .view(-1, 16, 3, 8, 8)  # Shape: [batch_size, num_patches, patch_dim]\n",
        "        # print(f\"patches after Rearrange has shape {patches.shape}\")\n",
        "\n",
        "        # Apply simclr_model.convnet() to each patch\n",
        "        processed_patches = []\n",
        "        for patch in patches.unbind(dim=1):\n",
        "            processed_patch = self.simclr_model.convnet(patch)\n",
        "            processed_patches.append(processed_patch)\n",
        "\n",
        "        processed_patches = torch.stack(processed_patches, dim=1) # Shape: [batch_size, num_patches, processed_patch_dim]\n",
        "        processed_patches = self.increase_dim(processed_patches)\n",
        "        processed_patches += self.pos_embedding.to(device, dtype=processed_patches.dtype)\n",
        "\n",
        "        x = self.transformer(processed_patches)\n",
        "        x = x.mean(dim=1)\n",
        "        x = self.to_latent(x)\n",
        "        return self.linear_head(x)\n"
      ],
      "metadata": {
        "id": "8ohLpjzHx8Jr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimCLRViT2Drop(nn.Module):\n",
        "    def __init__(self, simclr_model, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, channels = 3, dim_head = 64):\n",
        "        super().__init__()\n",
        "        image_height, image_width = pair(image_size)\n",
        "        patch_height, patch_width = pair(patch_size)\n",
        "        self.num_patches = (image_size // patch_size) ** 2\n",
        "\n",
        "        self.patch_size = patch_size\n",
        "        self.channels = channels\n",
        "        self.simclr_model = simclr_model\n",
        "\n",
        "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
        "\n",
        "        patch_dim = channels * patch_height * patch_width\n",
        "\n",
        "        self.to_patches = nn.Sequential(\n",
        "            Rearrange(\"b c (h p1) (w p2) -> b (h w) c p1 p2\", p1 = patch_height, p2 = patch_width),\n",
        "            # h x w == number of total patches, c == channels, b == number of batches\n",
        "            nn.LayerNorm(normalized_shape=(self.channels, self.patch_size, self.patch_size), elementwise_affine=True)\n",
        "            )\n",
        "\n",
        "        self.increase_dim = nn.Sequential(\n",
        "            nn.Linear(in_features=128, out_features=256),\n",
        "            nn.LayerNorm(256), # added\n",
        "        )\n",
        "\n",
        "        self.pos_embedding = posemb_sincos_2d(\n",
        "            h = image_height // patch_height,\n",
        "            w = image_width // patch_width,\n",
        "            dim = dim,\n",
        "        )\n",
        "\n",
        "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim)\n",
        "\n",
        "        self.pool = \"mean\"\n",
        "        self.to_latent = nn.Identity()\n",
        "\n",
        "        self.linear_head = nn.Linear(dim, num_classes)\n",
        "\n",
        "    def forward(self, img):\n",
        "        device = img.device # img has shape [b, 3, 32, 32], b = 64\n",
        "        patches = self.to_patches(img) # .view(-1, 16, 3, 8, 8)  # Shape: [batch_size, num_patches, patch_dim]\n",
        "        # print(f\"patches after Rearrange has shape {patches.shape}\")\n",
        "\n",
        "        # Apply simclr_model.convnet() to each patch\n",
        "        processed_patches = []\n",
        "        for patch in patches.unbind(dim=1):\n",
        "            processed_patch = self.simclr_model.convnet(patch)\n",
        "            processed_patches.append(processed_patch)\n",
        "\n",
        "        processed_patches = torch.stack(processed_patches, dim=1) # Shape: [batch_size, num_patches, processed_patch_dim]\n",
        "        processed_patches = self.increase_dim(processed_patches)\n",
        "        processed_patches += self.pos_embedding.to(device, dtype=processed_patches.dtype)\n",
        "        perm = torch.randperm(self.num_patches)\n",
        "\n",
        "        # Select num_patches // 2 patches for each image in the batch\n",
        "        selected_patches = processed_patches[:, perm[:self.num_patches // 4], :]\n",
        "\n",
        "        x = self.transformer(processed_patches)\n",
        "        x = x.mean(dim=1)\n",
        "        x = self.to_latent(x)\n",
        "        return self.linear_head(x)\n"
      ],
      "metadata": {
        "id": "Fl3Ibkpj-UyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimCLRViTDrop(nn.Module):\n",
        "  # Added simclr_encoder\n",
        "    def __init__(self, simclr_model, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, channels = 3, dim_head = 64):\n",
        "        super().__init__()\n",
        "        image_height, image_width = pair(image_size)\n",
        "        patch_height, patch_width = pair(patch_size)\n",
        "        self.simclr_model = simclr_model\n",
        "        self.num_patches = (image_size // patch_size) ** 2\n",
        "\n",
        "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
        "\n",
        "        patch_dim = channels * patch_height * patch_width\n",
        "\n",
        "        self.patching = nn.Sequential(\n",
        "            nn.Linear(128//self.num_patches, 256)\n",
        "        )\n",
        "\n",
        "        self.pos_embedding = posemb_sincos_2d(\n",
        "            h = image_height // patch_height,\n",
        "            w = image_width // patch_width,\n",
        "            dim = dim,\n",
        "        )\n",
        "\n",
        "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim)\n",
        "\n",
        "        self.pool = \"mean\"\n",
        "        self.to_latent = nn.Identity()\n",
        "\n",
        "        self.linear_head = nn.Linear(dim, num_classes)\n",
        "\n",
        "    def forward(self, img):\n",
        "        device = img.device\n",
        "\n",
        "        # Apply simclr encoder\n",
        "        x = self.simclr_model.convnet(img).view(-1, 128, 1)\n",
        "        b, _, _ = x.shape\n",
        "\n",
        "        # Segment the encoding as a \"patching\" operation\n",
        "        x = x.view(b, self.num_patches, -1)\n",
        "        x = self.patching(x)\n",
        "\n",
        "        x = x.view(-1, self.num_patches, 256)\n",
        "        perm = torch.randperm(self.num_patches)\n",
        "        x = x[:, perm[:self.num_patches // 4], :]\n",
        "        # print(f\"x post perm is {x.shape}\")\n",
        "        # Do not add position embedding\n",
        "        x = self.transformer(x)\n",
        "        x = x.mean(dim = 1)\n",
        "\n",
        "        x = self.to_latent(x)\n",
        "        return self.linear_head(x)"
      ],
      "metadata": {
        "id": "BiqHmFQREaki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgc0KsKsZ0wS",
        "outputId": "3958a539-ad2c-4f92-da5d-a12930f8a836"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "])\n",
        "\n",
        "train_dataset = datasets.STL10(root='./data', split='train', download=True, transform=transform)\n",
        "test_dataset = datasets.STL10(root='./data', split='test', download=True, transform=transform)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geD9B07gclF2",
        "outputId": "682c1576-6056-4d67-962b-10d8188c73b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 1.4333\n",
            "Epoch [2/10], Loss: 1.6067\n",
            "Epoch [3/10], Loss: 0.9650\n",
            "Epoch [4/10], Loss: 2.4216\n",
            "Epoch [5/10], Loss: 1.3538\n",
            "Epoch [6/10], Loss: 0.3398\n",
            "Epoch [7/10], Loss: 0.5787\n",
            "Epoch [8/10], Loss: 1.5520\n",
            "Epoch [9/10], Loss: 1.2613\n",
            "Epoch [10/10], Loss: 1.7118\n",
            "Test Accuracy: 59.15%\n"
          ]
        }
      ],
      "source": [
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Define data transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),  # Resize to CIFAR-10 image size\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),  # Normalize for CIFAR-10\n",
        "])\n",
        "\n",
        "# Instantiate the SimCLRViT model\n",
        "v = SimCLRViT(\n",
        "    simclr_model=simclr_model,\n",
        "    image_size=32,\n",
        "    patch_size=4,  # Adjust patch size for smaller images\n",
        "    num_classes=10,  # Number of STL10 classes\n",
        "    dim=256,  # Adjust the dimensionality of the model\n",
        "    depth=6,\n",
        "    heads=8,\n",
        "    mlp_dim=512\n",
        ").to(device)\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = torch.optim.Adam(v.parameters(), lr=3e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    v.train()\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = v(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Testing\n",
        "v.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = v(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f'Test Accuracy: {accuracy * 100:.2f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Define data transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "])\n",
        "\n",
        "# Instantiate the SimpleViT model\n",
        "v = SimpleViT(\n",
        "    image_size= 32, # 32, 256\n",
        "    patch_size=16, # 4, 32  # Adjust patch size for smaller images\n",
        "    num_classes=10,  # 1000 Number of STL10 classes\n",
        "    dim=256,# 256,  # 256, 1024 Adjust the dimensionality of the model\n",
        "    depth=6, #6\n",
        "    heads=8, # 8,16\n",
        "    mlp_dim=512 # 512,2048\n",
        ").to(device)\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = torch.optim.Adam(v.parameters(), lr=3e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    v.train()\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = v(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Testing\n",
        "v.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = v(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f'Test Accuracy: {accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o31nbvam7Jhg",
        "outputId": "d3fd1239-e055-4db7-ce41-86575cfb144c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 1.6393\n",
            "Epoch [2/10], Loss: 1.9787\n",
            "Epoch [3/10], Loss: 2.2211\n",
            "Epoch [4/10], Loss: 2.6683\n",
            "Epoch [5/10], Loss: 1.8880\n",
            "Epoch [6/10], Loss: 1.5890\n",
            "Epoch [7/10], Loss: 1.5190\n",
            "Epoch [8/10], Loss: 1.1467\n",
            "Epoch [9/10], Loss: 1.4744\n",
            "Epoch [10/10], Loss: 2.1445\n",
            "Test Accuracy: 35.59%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Define data transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "])\n",
        "\n",
        "# Instantiate the SimCLRViT2 model\n",
        "v = SimCLRViT2(\n",
        "    simclr_model=simclr_model,\n",
        "    image_size= 32, # 32, 256\n",
        "    patch_size=32, # 4, 16  # Adjust patch size for smaller images\n",
        "    num_classes=10,  # 1000 Number of STL10 classes\n",
        "    dim=256,# 256,  # 256, 1024 Adjust the dimensionality of the model\n",
        "    depth=6, #6\n",
        "    heads=8, # 8,16\n",
        "    mlp_dim=512 # 512,2048\n",
        ").to(device)\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = torch.optim.Adam(v.parameters(), lr=3e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    v.train()\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = v(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Testing\n",
        "v.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = v(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f'Test Accuracy: {accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpF0DQ9R2hbK",
        "outputId": "a176be01-1c15-4eea-b282-66a3192b0f71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "patch_height = 32, patch_width = 32\n",
            "img_height = 32, img_width = 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 1.7429\n",
            "Epoch [2/10], Loss: 1.2007\n",
            "Epoch [3/10], Loss: 0.8865\n",
            "Epoch [4/10], Loss: 1.8348\n",
            "Epoch [5/10], Loss: 1.7632\n",
            "Epoch [6/10], Loss: 0.8601\n",
            "Epoch [7/10], Loss: 1.0887\n",
            "Epoch [8/10], Loss: 1.3899\n",
            "Epoch [9/10], Loss: 2.0633\n",
            "Epoch [10/10], Loss: 2.5338\n",
            "Test Accuracy: 58.93%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Define data transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "])\n",
        "\n",
        "# Instantiate the SimCLRViT2 model\n",
        "v = SimCLRViT2Drop(\n",
        "    simclr_model=simclr_model,\n",
        "    image_size= 32, # 32, 256\n",
        "    patch_size=16, # 4, 16  # Adjust patch size for smaller images\n",
        "    num_classes=10,  # 1000 Number of STL10 classes\n",
        "    dim=256,# 256,  # 256, 1024 Adjust the dimensionality of the model\n",
        "    depth=6, #6\n",
        "    heads=8, # 8,16\n",
        "    mlp_dim=512 # 512,2048\n",
        ").to(device)\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = torch.optim.Adam(v.parameters(), lr=3e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    v.train()\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = v(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Testing\n",
        "v.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = v(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f'Test Accuracy: {accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "id": "on2j2nmyFebK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "456a6d49-00a5-43ad-9e3d-0bc510615017"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 3.3061\n",
            "Epoch [2/10], Loss: 0.9389\n",
            "Epoch [3/10], Loss: 2.9403\n",
            "Epoch [4/10], Loss: 0.5847\n",
            "Epoch [5/10], Loss: 1.4566\n",
            "Epoch [6/10], Loss: 1.8415\n",
            "Epoch [7/10], Loss: 2.0606\n",
            "Epoch [8/10], Loss: 0.8756\n",
            "Epoch [9/10], Loss: 2.4057\n",
            "Epoch [10/10], Loss: 2.5448\n",
            "Test Accuracy: 47.91%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Define data transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "])\n",
        "\n",
        "# Instantiate the SimCLRViT2 model\n",
        "v = SimCLRViTDrop(\n",
        "    simclr_model=simclr_model,\n",
        "    image_size= 32, # 32, 256\n",
        "    patch_size=16, # 4, 16  # Adjust patch size for smaller images\n",
        "    num_classes=10,  # 1000 Number of STL10 classes\n",
        "    dim=256,# 256,  # 256, 1024 Adjust the dimensionality of the model\n",
        "    depth=6, #6\n",
        "    heads=8, # 8,16\n",
        "    mlp_dim=512 # 512,2048\n",
        ").to(device)\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = torch.optim.Adam(v.parameters(), lr=3e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    v.train()\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = v(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Testing\n",
        "v.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = v(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f'Test Accuracy: {accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BW_J_lIWFyZp",
        "outputId": "b6810424-6a09-4f15-cf9f-b900b6df475b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.3675\n",
            "Epoch [2/10], Loss: 1.0045\n",
            "Epoch [3/10], Loss: 0.1432\n",
            "Epoch [4/10], Loss: 3.6719\n",
            "Epoch [5/10], Loss: 0.6380\n",
            "Epoch [6/10], Loss: 1.6379\n",
            "Epoch [7/10], Loss: 2.0503\n",
            "Epoch [8/10], Loss: 0.1290\n",
            "Epoch [9/10], Loss: 0.2755\n",
            "Epoch [10/10], Loss: 0.8163\n",
            "Test Accuracy: 61.08%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}